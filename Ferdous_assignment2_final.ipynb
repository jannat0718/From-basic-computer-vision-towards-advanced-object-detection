{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"17hzAyvpnUiXMGp566-LJFENe9C97VErd","authorship_tag":"ABX9TyNheWPpI8MQGlZXv2j/NGI1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Import required libraries"],"metadata":{"id":"YwOFYeAKyeJC"}},{"cell_type":"code","execution_count":26,"metadata":{"id":"WcFW-XiF7h0o","executionInfo":{"status":"ok","timestamp":1729284323694,"user_tz":-120,"elapsed":307,"user":{"displayName":"Jannatul Ferdous","userId":"17288488832087059331"}}},"outputs":[],"source":["from google.colab import files\n","from google.colab.patches import cv2_imshow\n","from IPython.display import Video\n","import argparse\n","import cv2\n","import sys\n","import numpy as np"]},{"cell_type":"markdown","source":["# All the funtions"],"metadata":{"id":"Oer9wuHJymLX"}},{"cell_type":"code","source":["# Helper function to check if the current time is between the given bounds\n","def between(cap, lower: int, upper: int) -> bool:\n","    return lower <= int(cap.get(cv2.CAP_PROP_POS_MSEC)) < upper\n"],"metadata":{"id":"l7b1Sjiw7t6p","executionInfo":{"status":"ok","timestamp":1729284328287,"user_tz":-120,"elapsed":13,"user":{"displayName":"Jannatul Ferdous","userId":"17288488832087059331"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# Gaussian kernel generator for Filterring\n","def gaussian_kernel(size: int, sigma: float) -> np.ndarray:\n","    # Create an empty kernel\n","    kernel = np.zeros((size, size), np.float32)\n","\n","    # Calculate the kernel values\n","    for x in range(size):\n","        for y in range(size):\n","            kernel[x, y] = (1 / (2 * np.pi * sigma ** 2)) * np.exp(\n","                -((x - (size - 1) / 2) ** 2 + (y - (size - 1) / 2) ** 2) / (2 * sigma ** 2)\n","            )\n","\n","    # Normalize the kernel so that the sum of all values is 1\n","    return kernel / np.sum(kernel)"],"metadata":{"id":"qlXIKOb2Xk09","executionInfo":{"status":"ok","timestamp":1729284328288,"user_tz":-120,"elapsed":12,"user":{"displayName":"Jannatul Ferdous","userId":"17288488832087059331"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["#ID card detection\n","def id_card_detection(frame):\n","    # Step 1: Convert the frame to grayscale\n","    gray_image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","    # Step 2: Apply Histogram Equalization\n","    equalized_image = cv2.equalizeHist(gray_image)\n","\n","    # Step 3: Apply binary thresholding to isolate white areas\n","    _, binary = cv2.threshold(equalized_image, 200, 255, cv2.THRESH_BINARY)\n","\n","    # Step 4: Use morphological operations (dilation and erosion)\n","    kernel = np.ones((5, 5), np.uint8)\n","    morph = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n","\n","    # Step 5: Find contours\n","    contours, _ = cv2.findContours(morph, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    # Step 6: Draw rectangles around detected contours and find the largest ID card region\n","    largest_contour = None\n","    largest_area = 0\n","\n","    for contour in contours:\n","        # Get the bounding box of each contour\n","        x, y, w, h = cv2.boundingRect(contour)\n","\n","        # Filter based on aspect ratio or size to ensure it's the ID card\n","        aspect_ratio = w / float(h)\n","        if 1.5 < aspect_ratio < 3.0:  # Adjust these values based on your ID card's expected shape\n","            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n","            # Calculate the area of the contour\n","            area = w * h\n","\n","            # Check if this is the largest area found\n","            if area > largest_area:\n","                largest_area = area\n","                largest_contour = contour\n","\n","    # Step 7: If a largest contour is found, extract the ID card region\n","    id_card_region = None\n","    if largest_contour is not None:\n","        x, y, w, h = cv2.boundingRect(largest_contour)\n","        id_card_region = frame[y:y + h, x:x + w]\n","\n","    return frame, id_card_region"],"metadata":{"id":"w-ViiQeA7zr8","executionInfo":{"status":"ok","timestamp":1729284331532,"user_tz":-120,"elapsed":296,"user":{"displayName":"Jannatul Ferdous","userId":"17288488832087059331"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# Temple matching function\n","def match_template(id_card_region, resized_template):\n","    # Ensure both images are in the correct format\n","    id_card_region = cv2.convertScaleAbs(id_card_region)\n","    resized_template = cv2.convertScaleAbs(resized_template)\n","\n","    # Convert only the resized template to grayscale for matching\n","    if len(resized_template.shape) == 3:\n","        resized_template_gray = cv2.cvtColor(resized_template, cv2.COLOR_BGR2GRAY)\n","    else:\n","        resized_template_gray = resized_template  # Already grayscale\n","\n","    # Check sizes before matching\n","    id_card_height, id_card_width = id_card_region.shape[:2]\n","    template_height, template_width = resized_template_gray.shape\n","\n","    if id_card_height < template_height or id_card_width < template_width:\n","        print(\"Error: Template size is larger than the ID card region size.\")\n","        return id_card_region  # Return the original id_card_region without matching\n","\n","    # Perform template matching using Normalized Cross-Correlation\n","    method = cv2.TM_CCOEFF_NORMED\n","    result = cv2.matchTemplate(cv2.cvtColor(id_card_region, cv2.COLOR_BGR2GRAY), resized_template_gray, method)\n","    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n","\n","    # Get the location of the best match\n","    h, w = template_height, template_width\n","    location = max_loc\n","    bottom_right = (location[0] + w, location[1] + h)\n","\n","    # Draw a rectangle around the matched region on the original color image\n","    cv2.rectangle(id_card_region, location, bottom_right, (0, 255, 0), 2)\n","\n","    return id_card_region\n"],"metadata":{"id":"knNdcSzsX2pA","executionInfo":{"status":"ok","timestamp":1729284335355,"user_tz":-120,"elapsed":398,"user":{"displayName":"Jannatul Ferdous","userId":"17288488832087059331"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# Draw function for optical flow\n","def draw_optical_flow_arrows(frame, flow, step=16):\n","    h, w = frame.shape[:2]\n","    y, x = np.mgrid[step//2:h:step, step//2:w:step].astype(int)\n","    # Extract the flow vectors\n","    fx, fy = flow[y, x].T\n","    frame_with_arrows = frame.copy()\n","\n","    # Draw the arrows on the frame\n","    for (x1, y1, dx, dy) in zip(x.flatten(), y.flatten(), fx.flatten(), fy.flatten()):\n","        # Calculate the endpoint of the arrow\n","        x2 = int(x1 + dx)\n","        y2 = int(y1 + dy)\n","        cv2.arrowedLine(frame_with_arrows, (x1, y1), (x2, y2), (255, 0, 0), 2, tipLength=0.5)\n","    return frame_with_arrows"],"metadata":{"id":"yPi848QQXDfc","executionInfo":{"status":"ok","timestamp":1729284338739,"user_tz":-120,"elapsed":7,"user":{"displayName":"Jannatul Ferdous","userId":"17288488832087059331"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["## Most image processing operations code here\n","def apply_processing(cap, frame, lower, upper, process_type, templates):\n","    \"\"\"Applies different image processing techniques based on the section of the video.\"\"\"\n","    processed_frame = frame.copy()  # Create a copy of the frame for processing\n","\n","    if between(cap, lower, upper):\n","        if process_type == \"blur\":\n","            # Apply Gaussian Blur\n","            processed_frame = cv2.GaussianBlur(frame, (7, 7), 0)\n","            cv2.putText(processed_frame, 'Smoothing using Gaussian Blur', (10, 30),\n","                        cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 2)\n","\n","        elif process_type == \"sharpen\":\n","            # Apply sharpening\n","            sharpen_kernel = np.array([[-1, -1, -1],\n","                                       [-1, 9, -1],\n","                                       [-1, -1, -1]])\n","            processed_frame = cv2.filter2D(frame, -1, sharpen_kernel)\n","            cv2.putText(processed_frame, 'Sharpening', (10, 30),\n","                        cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 2)\n","\n","        elif process_type == \"sobel\":\n","            # Apply Sobel edge detection\n","            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","            sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n","            sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n","            sobel_combined = cv2.magnitude(sobelx, sobely)\n","            processed_frame = cv2.cvtColor(sobel_combined.astype(np.uint8), cv2.COLOR_GRAY2BGR)\n","            cv2.putText(processed_frame, 'Sobel Edge Detection', (10, 30),\n","                        cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 2)\n","\n","        elif process_type == \"canny\":\n","            # Apply Canny edge detection\n","            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","            canny_edges = cv2.Canny(gray, 100, 200)\n","            processed_frame = cv2.cvtColor(canny_edges, cv2.COLOR_GRAY2BGR)\n","            cv2.putText(processed_frame, 'Canny Edge Detection (Thresholds: 100, 200)', (10, 30),\n","                        cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 2)\n","\n","        elif process_type == \"dft\":\n","            # Show DFT spectrum\n","            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","            dft = cv2.dft(np.float32(gray), flags=cv2.DFT_COMPLEX_OUTPUT)\n","            dft_shift = np.fft.fftshift(dft)\n","            magnitude = cv2.magnitude(dft_shift[:, :, 0], dft_shift[:, :, 1])\n","\n","            # Normalize the magnitude to the range [0, 255]\n","            magnitude = np.log1p(magnitude)  # Use log scale for better visualization\n","            magnitude = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n","            magnitude = np.uint8(magnitude)\n","\n","            # Convert back to BGR for displaying\n","            magnitude_bgr = cv2.cvtColor(magnitude, cv2.COLOR_GRAY2BGR)\n","            cv2.putText(magnitude_bgr, 'DFT Magnitude Spectrum', (10, 30),\n","                        cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 2)\n","\n","            processed_frame = magnitude_bgr  # Set the processed frame to the DFT display\n","\n","        elif process_type == \"fourier_low\":\n","            # Apply Gaussian Low Pass Filter\n","            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","            kernel_size = 15\n","            sigma = 15\n","            kernel = gaussian_kernel(kernel_size, sigma)\n","\n","            processed_frame = cv2.filter2D(gray, -1, kernel)\n","            processed_frame = cv2.normalize(processed_frame, None, 0, 255, cv2.NORM_MINMAX)\n","            processed_frame = np.uint8(processed_frame)\n","            processed_frame = cv2.cvtColor(processed_frame, cv2.COLOR_GRAY2BGR)\n","\n","            cv2.putText(processed_frame, 'Gaussian Low Pass Filter', (10, 30),\n","                        cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 2)\n","\n","        elif process_type == \"fourier_high\":\n","            # Apply Gaussian High Pass Filter\n","            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","            # Create a Gaussian kernel for low-pass filtering\n","            kernel_size = 9\n","            sigma = 9\n","            low_pass_kernel = gaussian_kernel(kernel_size, sigma)\n","\n","            # Create a high-pass kernel by subtracting low-pass kernel from an identity kernel\n","            high_pass_kernel = np.zeros((kernel_size, kernel_size), np.float32)\n","            high_pass_kernel[kernel_size // 2, kernel_size // 2] = 1\n","            high_pass_kernel -= low_pass_kernel\n","\n","            # Apply the high-pass filter using convolution\n","            processed_frame = cv2.filter2D(gray, -1, high_pass_kernel)\n","\n","            # Normalize the result\n","            processed_frame = cv2.normalize(processed_frame, None, 0, 255, cv2.NORM_MINMAX)\n","            processed_frame = np.uint8(processed_frame)\n","\n","            # Convert back to BGR\n","            processed_frame = cv2.cvtColor(processed_frame, cv2.COLOR_GRAY2BGR)\n","\n","            # Add title for high-pass filter\n","            cv2.putText(processed_frame, 'Gaussian High Pass Filter', (10, 30),\n","                        cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 2)\n","\n","        elif process_type == \"fourier_band\":\n","            # Apply Gaussian Band Pass Filter\n","            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","            # Create Gaussian Low-Pass Kernel\n","            low_pass_size = 15\n","            low_pass_sigma = 15\n","            low_pass_kernel = gaussian_kernel(low_pass_size, low_pass_sigma)\n","\n","            # Create Gaussian High-Pass Kernel\n","            high_pass_size = 15\n","            high_pass_sigma = 5\n","            high_pass_kernel = gaussian_kernel(high_pass_size, high_pass_sigma)\n","\n","            # Apply the low-pass filter\n","            low_passed = cv2.filter2D(gray, -1, low_pass_kernel)\n","\n","            # Apply the high-pass filter\n","            high_passed = cv2.filter2D(gray, -1, high_pass_kernel)\n","\n","            # Combine the results to create a band-pass effect\n","            processed_frame = cv2.addWeighted(low_passed, 1, high_passed, -1, 0)\n","\n","            # Normalize the result\n","            processed_frame = cv2.normalize(processed_frame, None, 0, 255, cv2.NORM_MINMAX)\n","            processed_frame = np.uint8(processed_frame)\n","            processed_frame = cv2.cvtColor(processed_frame, cv2.COLOR_GRAY2BGR)\n","\n","            cv2.putText(processed_frame, 'Gaussian Band Pass Filter', (10, 30),\n","                        cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 2)\n","\n","        elif process_type == \"template_matching_s_number\":\n","            # Check if templates are provided\n","            if templates is not None:\n","                # Perform template matching for s-number\n","                processed_frame = match_template(frame, templates['s_number'])\n","                cv2.putText(processed_frame, 'Student number', (10, 30),\n","                            cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 2)\n","\n","        elif process_type == \"template_matching_logo\":\n","            # Check if templates are provided\n","            if templates is not None:\n","                # Perform template matching for logo\n","                processed_frame = match_template(frame, templates['logo'])\n","                cv2.putText(processed_frame, 'UT Logo', (10, 30),\n","                            cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 2)\n","\n","        elif process_type == \"template_matching_photo\":\n","            # Check if templates are provided\n","            if templates is not None:\n","                # Perform template matching for photo\n","                processed_frame = match_template(frame, templates['photo'])\n","                cv2.putText(processed_frame, 'ID photo', (10, 30),\n","                            cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 2)\n","\n","        elif process_type == \"optical flow\":\n","            # Initialize previous grayscale frame if not available\n","            if not hasattr(apply_processing, \"prev_gray\"):\n","                apply_processing.prev_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","            # Convert current frame to grayscale\n","            curr_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","            # Check if sizes match before calculating optical flow\n","            if apply_processing.prev_gray.shape == curr_gray.shape:\n","                # Calculate optical flow (Farneback method)\n","                flow = cv2.calcOpticalFlowFarneback(apply_processing.prev_gray, curr_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n","\n","                # Draw optical flow arrows\n","                processed_frame = draw_optical_flow_arrows(frame, flow)\n","\n","                # Update previous grayscale image for the next frame\n","                apply_processing.prev_gray = curr_gray\n","\n","        elif process_type == \"laplacian\":\n","            # Apply Laplacian filter\n","            processed_frame = cv2.Laplacian(frame, cv2.CV_64F)\n","            processed_frame = cv2.convertScaleAbs(processed_frame)\n","            cv2.putText(processed_frame, 'Laplacian Filter', (10, 30),\n","                        cv2.FONT_HERSHEY_SIMPLEX, .5, (255, 255, 255), 2)\n","\n","        elif process_type == \"color_map\":\n","            # Apply a color map\n","            processed_frame = cv2.applyColorMap(frame, cv2.COLORMAP_JET)\n","            cv2.putText(processed_frame, 'Color Map', (10, 30),\n","                        cv2.FONT_HERSHEY_SIMPLEX, .5, (255, 255, 255), 2)\n","\n","        elif process_type == \"negative\":\n","            # Invert colors\n","            processed_frame = cv2.bitwise_not(frame)\n","            cv2.putText(processed_frame, 'Negative', (10, 30),\n","                        cv2.FONT_HERSHEY_SIMPLEX, .5, (255, 255, 255), 2)\n","\n","        elif process_type == \"adaptive_threshold\":\n","            # Apply adaptive threshold\n","            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","            processed_frame = cv2.adaptiveThreshold(gray_frame, 255,\n","                                                    cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n","                                                    cv2.THRESH_BINARY, 11, 2)\n","            cv2.putText(processed_frame, 'Adaptive Threshold', (10, 30),\n","                        cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 2)\n","\n","        elif process_type == \"rotation\":\n","            # Rotate the frame by a specified angle (e.g., 45 degrees)\n","            angle = 45\n","            center = (frame.shape[1] // 2, frame.shape[0] // 2)\n","            rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n","            processed_frame = cv2.warpAffine(frame, rotation_matrix, (frame.shape[1], frame.shape[0]))\n","            cv2.putText(processed_frame, 'Rotation', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 2)\n","\n","        elif process_type == \"affine\":\n","            # Apply affine transformation\n","            rows, cols = frame.shape[:2]\n","            points1 = np.float32([[50, 50], [200, 50], [50, 200]])\n","            points2 = np.float32([[10, 100], [200, 50], [100, 250]])\n","            affine_matrix = cv2.getAffineTransform(points1, points2)\n","            processed_frame = cv2.warpAffine(frame, affine_matrix, (cols, rows))\n","            cv2.putText(processed_frame, 'Affine Transform', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 2)\n","\n","    return processed_frame"],"metadata":{"id":"NUsijb3Co2Uk","executionInfo":{"status":"ok","timestamp":1729284342483,"user_tz":-120,"elapsed":295,"user":{"displayName":"Jannatul Ferdous","userId":"17288488832087059331"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["## vedio frame process, time management operations\n","def main(input_video_file: str, output_video_file: str) -> None:\n","    # OpenCV video objects to work with\n","    cap = cv2.VideoCapture(input_video_file)\n","    if not cap.isOpened():\n","        print(\"Error: Could not open video.\")\n","        return\n","\n","    fps = int(round(cap.get(cv2.CAP_PROP_FPS)))\n","    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Saving output video as .mp4\n","    out = cv2.VideoWriter(output_video_file, fourcc, fps, (frame_width, frame_height))\n","\n","    frame_count = 0  # Counter for the number of frames processed\n","\n","    id_card_region = None  # Initialize ID card region variable\n","\n","    # While loop where the real work happens\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break  # Break if frame read fails\n","\n","        frame_count += 1  # Increment frame count\n","\n","        # Calculate current time in milliseconds\n","        current_time_ms = frame_count * 1000 // fps  # Convert frame count to milliseconds\n","\n","        # Detect ID card region only once for relevant sections\n","        if 20000 <= current_time_ms < 40000:\n","            frame, id_card_region = id_card_detection(frame)\n","\n","        # Apply different effects based on the timestamp\n","        if between(cap, 0, 2500):\n","            frame = apply_processing(cap, frame, 0, 2500, \"blur\", templates)\n","        elif between(cap, 2500, 5000):\n","            frame = apply_processing(cap, frame, 2500, 5000, \"sharpen\", templates)\n","        elif between(cap, 5000, 7500):\n","            frame = apply_processing(cap, frame, 5000, 7500, \"sobel\", templates)\n","        elif between(cap, 7500, 10000):\n","            frame = apply_processing(cap, frame, 7500, 10000, \"canny\", templates)\n","        elif between(cap, 10000, 12500):\n","            frame = apply_processing(cap, frame, 10000, 12500, \"dft\", templates)\n","        elif between(cap, 12500, 15000):\n","            frame = apply_processing(cap, frame, 12500, 15000, \"fourier_low\", templates)\n","        elif between(cap, 15000, 17500):\n","            frame = apply_processing(cap, frame, 15000, 17500, \"fourier_high\", templates)\n","        elif between(cap, 17500, 20000):\n","            frame = apply_processing(cap, frame, 17500, 20000, \"fourier_band\", templates)\n","        elif 20000 <= current_time_ms < 25000 and id_card_region is not None:\n","            frame = apply_processing(cap, frame, 20000, 25000, \"template_matching_s_number\", templates)\n","        elif 25000 <= current_time_ms < 30000 and id_card_region is not None:\n","            frame = apply_processing(cap, frame, 25000, 30000, \"template_matching_logo\", templates)\n","        elif 30000 <= current_time_ms < 35000 and id_card_region is not None:\n","            frame = apply_processing(cap, frame, 30000, 35000, \"template_matching_photo\", templates)\n","        elif 35000 <= current_time_ms < 40000 and id_card_region is not None:\n","            frame = apply_processing(cap, frame, 35000, 40000, \"optical flow\", templates)\n","        # Apply different effects based on the timestamp\n","        elif between(cap, 40000, 43000):\n","            frame = apply_processing(cap, frame, 40000, 43000, \"affine\", templates)\n","        elif between(cap, 43000, 46000):\n","            frame = apply_processing(cap, frame, 43000, 46000, \"color_map\", templates)\n","        elif between(cap, 46000, 49000):\n","            frame = apply_processing(cap, frame, 46000, 49000, \"negative\", templates)\n","        elif between(cap, 49000, 52000):\n","            frame = apply_processing(cap, frame, 49000, 52000, \"laplacian\", templates)\n","        elif between(cap, 52000, 55000):\n","            frame = apply_processing(cap, frame, 52000, 55000, \"adaptive_threshold\", templates)\n","        elif between(cap, 55000, 58000):\n","            frame = apply_processing(cap, frame, 55000, 58000, \"rotation\", templates)\n","\n","        # Write processed frame to output\n","        out.write(frame)\n","\n","        # Display every 30th frame using cv2_imshow\n","        if frame_count % 30 == 0:\n","            cv2_imshow(frame)\n","\n","        # Optional: Press Q on keyboard to exit\n","        if cv2.waitKey(25) & 0xFF == ord('q'):\n","            break\n","\n","    # When everything is done, release the video capture and writing object\n","    cap.release()\n","    out.release()\n","\n"],"metadata":{"id":"YusrpxWmo2mP","executionInfo":{"status":"ok","timestamp":1729284350712,"user_tz":-120,"elapsed":962,"user":{"displayName":"Jannatul Ferdous","userId":"17288488832087059331"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["# Template images input and sizing"],"metadata":{"id":"8ygHuXxFzVX8"}},{"cell_type":"code","source":["# Load templates for s-number, logo, and unique pattern\n","template_s_number = cv2.imread('/content/drive/MyDrive/Image processing assignments /Ferdous_assignment2/S_number_200.png', cv2.IMREAD_GRAYSCALE)\n","# Set a desired width for the resized template\n","desired_width_snumber = 150\n","aspect_ratio_snumber = template_s_number.shape[1] / template_s_number.shape[0]\n","desired_height_snumber = int(desired_width_snumber / aspect_ratio_snumber)\n","resized_template_snumber = cv2.resize(template_s_number, (desired_width_snumber, desired_height_snumber))\n","\n","# Load templates for logo\n","template_logo = cv2.imread('/content/drive/MyDrive/Image processing assignments /Ferdous_assignment2/logo_resized.jpg', cv2.IMREAD_GRAYSCALE)\n","desired_width_logo = 350\n","aspect_ratio_logo = template_logo.shape[1] / template_logo.shape[0]\n","desired_height_logo = int(desired_width_logo / aspect_ratio_logo)\n","resized_template_logo = cv2.resize(template_logo, (desired_width_logo, desired_height_logo))\n","\n","\n","# Load templates for photo\n","template_photo = cv2.imread('/content/drive/MyDrive/Image processing assignments /Ferdous_assignment2/photo.png', cv2.IMREAD_GRAYSCALE)\n","desired_width_photo = 175\n","aspect_ratio_photo = template_photo.shape[1] / template_photo.shape[0]\n","desired_height_photo = int(desired_width_photo / aspect_ratio_photo)\n","resized_template_photo = cv2.resize(template_photo, (desired_width_photo, desired_height_photo))\n","\n","# Assuming `templates` is defined as:\n","templates = {\n","    's_number': resized_template_snumber,\n","    'logo': resized_template_logo,\n","    'photo': resized_template_photo\n","}\n"],"metadata":{"id":"4wV6TKlU8PP6","executionInfo":{"status":"ok","timestamp":1729284412527,"user_tz":-120,"elapsed":260,"user":{"displayName":"Jannatul Ferdous","userId":"17288488832087059331"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["# Input and Output"],"metadata":{"id":"9c_c4SlXzlPj"}},{"cell_type":"code","source":["# Define the input and output file paths\n","input_video_file = '/content/drive/MyDrive/Image processing assignments /Ferdous_assignment2/Ferdous_1_60sec.mp4'  # Input video path\n","output_video_file = '/content/drive/MyDrive/Image processing assignments /Ferdous_assignment2/output_video_1_60.mp4'  # Output video path\n","\n","main(input_video_file, output_video_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"13uEtdClN0Wl9PEo8BjRR_9o_9ol-IsEh"},"id":"sAz6PZAM8UKM","executionInfo":{"status":"ok","timestamp":1729284607396,"user_tz":-120,"elapsed":123816,"user":{"displayName":"Jannatul Ferdous","userId":"17288488832087059331"}},"outputId":"584d2d96-e785-48ff-d1bb-bd23259533b3"},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}